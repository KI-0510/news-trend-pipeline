name: Daily Pipeline

permissions:
  contents: write

on:
  schedule:
    - cron: "0 21 * * *" # UTC 21:00 = KST 06:00
  workflow_dispatch:
    inputs:
      dry_run:
        description: "드라이 런"
        required: true
        default: "true"
      pro_mode:
        description: "Pro 모드로 실행할까요? (true/false)"
        required: false
        default: "false"

concurrency:
  group: daily-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    env:
      USE_PRO: ${{ github.event_name == 'workflow_dispatch' && inputs.pro_mode || 'false' }}
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      HF_HOME: ~/.cache/huggingface

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "requirements.txt"

      - name: Install dependencies
        shell: bash
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      - name: Prepare HuggingFace cache (optional)
        if: env.USE_PRO == 'true'
        shell: bash
        run: |
          mkdir -p "${HF_HOME}"
          echo "HF_HOME=${HF_HOME}"
      
      - name: Install viz deps
        shell: bash
        run: |
          source .venv/bin/activate
          sudo apt-get update
          sudo apt-get install -y fonts-nanum fonts-noto-cjk
          fc-cache -f -v

      - name: Preflight | Check secrets
        shell: bash
        run: |
          test -n "${{ secrets.GEMINI_API_KEY }}" || (echo "GEMINI_API_KEY 없음"; exit 1)
          if [ -z "${{ secrets.NAVER_CLIENT_ID }}" ] || [ -z "${{ secrets.NAVER_CLIENT_SECRET }}" ]; then
            echo "[WARN] NAVER API 키가 없습니다(옵션?)."
          fi

      - name: Module A - Fetch & Preprocess
        shell: bash
        env:
          NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
          NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
          DRY_RUN: ${{ github.event_name == 'workflow_dispatch' && inputs.dry_run || 'false' }}
        run: |
          source .venv/bin/activate
          python -m src.module_a

      - name: Check A - Validate Output
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.check_a

      - name: Warehouse Append
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.warehouse_append

      - name: Commit & Push Warehouse
        shell: bash
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/warehouse
          git commit -m "chore: warehouse append $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true
      
      - name: Enrich meta with article bodies
        shell: bash
        env:
          BODY_MIN_LEN: "200"
        run: |
          source .venv/bin/activate
          python -m scripts.fetch_article_bodies
          
      - name: Copy latest meta file for debugging
        shell: bash
        run: |
          set -e
          mkdir -p outputs/debug
          LATEST_META=$(ls -t data/news_meta_*.json | head -n 1)
          if [ -f "$LATEST_META" ]; then
            echo "Copying $LATEST_META to outputs/debug/news_meta_latest.json"
            cp "$LATEST_META" outputs/debug/news_meta_latest.json
          else
            echo "WARN: No news_meta_*.json file found to copy."
          fi

      - name: Module B - Keywords
        shell: bash
        run: |
          source .venv/bin/activate
          echo "[INFO] USE_PRO=${USE_PRO} → Module B 실행(워크플로우)"
          python -m src.module_b

      - name: Check B - Validate Keywords
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.check_b

      - name: Module C - Topics/Timeseries/Insight
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          source .venv/bin/activate
          echo "[INFO] USE_PRO=${USE_PRO} → Module C 실행(워크플로우)"
          python -m src.module_c

      - name: Check C - Validate Insights
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.check_c

      - name: Export signals (trend strength & weak signals)
        shell: bash
        run: |
          source .venv/bin/activate
          python -m scripts.signal_export
          
      - name: Module D - Biz Opportunities
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          source .venv/bin/activate
          python -m src.module_d

      - name: Check D - Validate Biz Opportunities
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.check_d

      - name: Preflight Checks
        shell: bash
        env:
          PREFLIGHT_MIN_DAILY: "1"
          PREFLIGHT_MIN_TOTAL: "1"
          PREFLIGHT_MAX_SPAN:  "400"
        run: |
          set -e
          source .venv/bin/activate
          python -m scripts.preflight
          
      - name: Module E - Build Report
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.module_e
          
      - name: Check E - Validate Report
        shell: bash
        run: |
          source .venv/bin/activate
          python -m src.check_e
      
      # Job Summary
      - name: Build Job Summary
        shell: bash
        run: |
          source .venv/bin/activate
          python - <<'PY'
          import json, os
          def load(p, default):
              try:
                  with open(p, encoding="utf-8") as f:
                      return json.load(f)
              except Exception:
                  return default
          ts = load("outputs/trend_timeseries.json", {"daily":[]})
          daily = ts.get("daily", [])
          total = sum(int(x.get("count",0)) for x in daily)
          dr = f"{daily[0]['date']} ~ {daily[-1]['date']}" if daily else "-"
          kw = load("outputs/keywords.json", {"keywords":[]}).get("keywords", [])
          kw = sorted(kw, key=lambda x: x.get("score",0), reverse=True)[:10]
          opp = load("outputs/biz_opportunities.json", {"ideas":[]}).get("ideas", [])[:5]
          lines = []
          lines.append("# Daily Pipeline Summary\n")
          lines.append(f"- 기간: {dr}")
          lines.append(f"- 총 기사 수: {total}\n")
          lines.append("## Top 10 Keywords")
          lines.append("| Rank | Keyword | Score |")
          lines.append("|---:|---|---:|")
          for i, k in enumerate(kw, 1):
              lines.append(f"| {i} | {k.get('keyword','')} | {round(float(k.get('score',0)),3)} |")
          lines.append("")
          lines.append("## Opportunities (Top 5)")
          if opp:
              for i, it in enumerate(opp, 1):
                  title = it.get('title') or it.get('idea') or '(no title)'
                  lines.append(f"- {i}. {title}")
          else:
              lines.append("- (데이터 없음)")
          txt = "\n".join(lines) + "\n"
          print(txt)
          summ = os.environ.get("GITHUB_STEP_SUMMARY")
          if summ:
              with open(summ, "a", encoding="utf-8") as f:
                  f.write(txt)
          PY

      # 일자별 아카이브(KST)
      - name: Prepare outputs daily folder (KST)
        shell: bash
        run: |
          set -e
          DATE_KST=$(TZ=Asia/Seoul date +'%Y-%m-%d')
          TIME_KST=$(TZ=Asia/Seoul date +'%H%M-KST')
          OUTDIR="outputs/daily/${DATE_KST}/${TIME_KST}"
          mkdir -p "${OUTDIR}/fig" "${OUTDIR}/debug" "${OUTDIR}/export"

          # 기본 결과 파일 복사 (json, html, md)
          cp outputs/*.json             "${OUTDIR}/" || true
          cp outputs/*.html             "${OUTDIR}/" || true
          cp outputs/*.md               "${OUTDIR}/" || true

          # 디버그 메타 파일들을 올바른 위치(debug/)에서 상위 폴더로 복사
          if [ -f outputs/debug/run_meta_b.json ]; then
            cp outputs/debug/run_meta_b.json "${OUTDIR}/"
          fi
          if [ -f outputs/debug/run_meta_c.json ]; then
            cp outputs/debug/run_meta_c.json "${OUTDIR}/"
          fi
          
          # 서브폴더 내용 전체 복사 (export, fig, debug)
          if [ -d outputs/export ]; then
            cp -r outputs/export/* "${OUTDIR}/export/" || true
          fi
          if [ -d outputs/fig ]; then
            cp -r outputs/fig/* "${OUTDIR}/fig/" || true
          fi
          if [ -d outputs/debug ]; then
            cp -r outputs/debug/* "${OUTDIR}/debug/" || true
          fi
          
          echo "OUTDIR=${OUTDIR}" >> $GITHUB_ENV

      # outputs 커밋/푸시
      - name: Commit & Push outputs
        shell: bash
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add outputs/daily
          git commit -m "chore: outputs (KST rotation)" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true

      # 오래된 warehouse 정리
      - name: Cleanup old warehouse (main only, keep 90 days)
        if: github.ref_name == 'main'
        shell: bash
        run: |
          set -e
          test -d data/warehouse || exit 0
          find data/warehouse -type f -daystart -mtime +90 -print -delete || true
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A data/warehouse
          git commit -m "chore: cleanup old warehouse (>90d)" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true
          
      # 오래된 outputs 정리
      - name: Cleanup old outputs (main only, keep 90 days)
        if: github.ref_name == 'main'
        shell: bash
        run: |
          set -e
          test -d outputs/daily || exit 0
          find outputs/daily -mindepth 1 -maxdepth 1 -type d -daystart -mtime +90 -print -exec rm -rf {} \; || true
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A outputs/daily
          git commit -m "chore: cleanup old outputs (>90d)" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true

      # 아티팩트 이름 안전화
      - name: Sanitize artifact name
        shell: bash
        run: |
          CLEAN=$(echo '${{ github.ref_name }}' | tr '/:*?"<>|\\ ' '-')
          echo "ARTIFACT_NAME=report-${CLEAN}" >> $GITHUB_ENV

      # 결과 아티팩트 업로드(메타/클린/디버그 포함)
#      - name: Upload report artifacts
#        if: success()
#        uses: actions/upload-artifact@v4
#        with:
#          name: ${{ env.ARTIFACT_NAME }}
#          retention-days: 14
#          path: |
#            outputs/report.html
#            outputs/report.md
#            outputs/fig/*.png
#            outputs/export/*.csv
#            outputs/debug/*.json
#            outputs/debug/run_meta_b.json
#            outputs/debug/run_meta_c.json
#            data/news_meta_*.json
#            data/news_clean_*.json
#          if-no-files-found: warn
